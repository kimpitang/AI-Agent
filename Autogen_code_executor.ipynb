{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc9P8ZTn9L0IOlc0KWlvcz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimpitang/AI-Agent/blob/main/Autogen_code_executor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyautogen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYZxGzpjeKrm",
        "outputId": "4df1802f-53d5-41ca-ca2c-19f7454292d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen\n",
            "  Downloading pyautogen-0.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting asyncer==0.0.8 (from pyautogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from pyautogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from pyautogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fast-depends<3,>=2.4.12 (from pyautogen)\n",
            "  Downloading fast_depends-2.4.12-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting flaml (from pyautogen)\n",
            "  Downloading FLAML-2.3.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.58 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.59.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pyautogen) (24.2)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.10.4)\n",
            "Collecting python-dotenv (from pyautogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.5.0)\n",
            "Collecting tiktoken (from pyautogen)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: websockets<15,>=14 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (14.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from asyncer==0.0.8->pyautogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.58->pyautogen) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.27.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.4.0->asyncer==0.0.8->pyautogen) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.4.0->asyncer==0.0.8->pyautogen) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.58->pyautogen) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->pyautogen) (3.4.1)\n",
            "Downloading pyautogen-0.7.0-py3-none-any.whl (497 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.3/497.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading fast_depends-2.4.12-py3-none-any.whl (17 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FLAML-2.3.3-py3-none-any.whl (314 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.2/314.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, flaml, diskcache, tiktoken, docker, asyncer, fast-depends, pyautogen\n",
            "Successfully installed asyncer-0.0.8 diskcache-5.6.3 docker-7.1.0 fast-depends-2.4.12 flaml-2.3.3 pyautogen-0.7.0 python-dotenv-1.0.1 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "perlp1jzcQhB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['OPENAI_API_KEY'] = \"\" // openai api key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "from autogen.coding import LocalCommandLineCodeExecutor\n",
        "\n",
        "config_list = [{\"model\": \"gpt-4o-mini\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]"
      ],
      "metadata": {
        "id": "cozFCqtbcz9i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = autogen.AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    llm_config={\n",
        "        \"config_list\": config_list,\n",
        "        \"temperature\": 0,\n",
        "    },\n",
        ")\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    code_execution_config={\n",
        "        # the executor to run the generated code\n",
        "        \"executor\": LocalCommandLineCodeExecutor(work_dir=\"config\"),\n",
        "    },\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "chat_res = user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"\"\"100까지 소수가 어떤 것이 있는지 코드로 살펴봐주세요.\"\"\",\n",
        "    summary_method=\"reflection_with_llm\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88CgA7yvc0RB",
        "outputId": "a8283a57-8cd5-4e1a-89e3-4a883159e039"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "100까지 소수가 어떤 것이 있는지 코드로 살펴봐주세요.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "소수를 찾기 위해 1부터 100까지의 숫자를 검사하는 파이썬 코드를 작성하겠습니다. 이 코드는 소수를 판별하고, 100 이하의 모든 소수를 출력할 것입니다.\n",
            "\n",
            "아래 코드를 실행해 주세요.\n",
            "\n",
            "```python\n",
            "# filename: find_primes.py\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    for i in range(2, int(n**0.5) + 1):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "\n",
            "primes = [num for num in range(1, 101) if is_prime(num)]\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "코드를 실행한 후 결과를 알려주세요.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "100 이하의 소수는 다음과 같습니다:\n",
            "\n",
            "\\[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97\\]\n",
            "\n",
            "작업이 완료되었습니다. TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# followup of the previous question\n",
        "user_proxy.send(\n",
        "    recipient=assistant,\n",
        "    message=\"\"\"예시 영어 문장을 만들고, 키워드를 추출하세요\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC0tVG4Nc0e9",
        "outputId": "52c1be72-8d28-49ec-fa3b-0705b1ecf4e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "예시 영어 문장을 만들고, 키워드를 추출하세요\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "이번에는 NLTK 라이브러리를 사용하지 않고, 간단한 방법으로 예시 영어 문장에서 키워드를 추출하는 코드를 작성하겠습니다. 아래 코드는 기본적인 문자열 처리 방법을 사용하여 키워드를 추출합니다.\n",
            "\n",
            "예시 문장: \"The quick brown fox jumps over the lazy dog.\"\n",
            "\n",
            "아래 코드를 실행해 주세요.\n",
            "\n",
            "```python\n",
            "# filename: extract_keywords_simple.py\n",
            "import re\n",
            "\n",
            "# 예시 문장\n",
            "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
            "\n",
            "# 소문자로 변환하고, 알파벳 문자만 남기기\n",
            "words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
            "\n",
            "# 불용어 리스트\n",
            "stop_words = set([\"the\", \"is\", \"in\", \"and\", \"to\", \"a\", \"of\", \"over\", \"jumps\", \"lazy\"])\n",
            "\n",
            "# 키워드 추출\n",
            "keywords = [word for word in words if word not in stop_words]\n",
            "\n",
            "print(keywords)\n",
            "```\n",
            "\n",
            "코드를 실행한 후 결과를 알려주세요.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: ['quick', 'brown', 'fox', 'dog']\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "키워드 추출 결과는 다음과 같습니다:\n",
            "\n",
            "\\[ 'quick', 'brown', 'fox', 'dog' \\]\n",
            "\n",
            "작업이 완료되었습니다. TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}