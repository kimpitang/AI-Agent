{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQeoKEeBY8aAJfoBAzIWC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimpitang/AI-Agent/blob/main/LangChain%EC%9C%BC%EB%A1%9C_%EA%B0%84%EB%8B%A8%ED%95%9C_Chain_%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kWfmto7CUHX",
        "outputId": "0f861dd1-c32f-419f-dfc7-a08145148a46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.33 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.59.9)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_openai) (0.3.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_openai) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_openai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain_openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain_openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.32\n",
            "    Uninstalling langchain-core-0.3.32:\n",
            "      Successfully uninstalled langchain-core-0.3.32\n",
            "Successfully installed langchain-core-0.3.33 langchain_openai-0.3.3 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r3IRdk8Y4FRK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY']= \"\" // 개인 API KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 컴포넌트 실습"
      ],
      "metadata": {
        "id": "TF7lStgd4_5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"2002년 월드컵 4강 국가 알려줘\"\n",
        "        }\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORJwsDiw5EIK",
        "outputId": "5b64ebfd-8729-4e89-de5f-f1bc878e50f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-AwfSE7yg8ftUyLZ2M68BmGZMiYLFr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='2002년 월드컵 4강에 진출한 국가는 브라질, 독일, 터키, 그리고 대한민국입니다.결승전에서 브라질과 독일이 만나 독일이 2-0으로 우승하였습니다.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738545738, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=89, prompt_tokens=31, total_tokens=120, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "chat = ChatOpenAI(\n",
        "    model_name='gpt-4o-mini'\n",
        ")\n",
        "chat.invoke(\"안녕~ 너를 소개해줄래?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dsJODeV-luG",
        "outputId": "fcb9c765-d35c-4ac9-aa04-0f5c6e906fa9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='안녕하세요! 저는 OpenAI에서 개발한 AI 언어 모델인 ChatGPT입니다. 다양한 질문에 답하고, 정보를 제공하며, 대화에 참여할 수 있도록 설계되었습니다. 학습된 내용을 바탕으로 여러 가지 주제에 대해 이야기할 수 있지만, 감정이나 의식은 없답니다. 무엇이든 궁금한 점이 있다면 말씀해 주세요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 17, 'total_tokens': 99, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-e582f737-434a-4d44-8af0-5fab0952c803-0', usage_metadata={'input_tokens': 17, 'output_tokens': 82, 'total_tokens': 99, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PromptTemplate 실습"
      ],
      "metadata": {
        "id": "vfngrDR5Cb5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 {개수}추천하고,\n",
        "        그 요리의 레시피를 제시해줘. 내가 가진 재료는 아래와 같아.\n",
        "        <재료>\n",
        "        {재료}\n",
        "        \"\"\"\n",
        "    )\n",
        ")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1wuU7LFCgoH",
        "outputId": "6927593a-d228-4c62-fcd8-a31d6b3e2075"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['개수', '재료'], input_types={}, partial_variables={}, template='\\n        너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 {개수}추천하고,\\n        그 요리의 레시피를 제시해줘. 내가 가진 재료는 아래와 같아.\\n        <재료>\\n        {재료}\\n        ')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.invoke({\"개수\":6, \"재료\":\"사과, 잼\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP-aOh_lD_ym",
        "outputId": "591dab42-2425-4ee7-8134-5d17d490d6f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='\\n        너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 6추천하고,\\n        그 요리의 레시피를 제시해줘. 내가 가진 재료는 아래와 같아.\\n        <재료>\\n        사과, 잼\\n        ')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "    #SystemMessage: 유용한 챗봇이라는 역할과 이름을 부여\n",
        "      (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    #HumanMessage와 AIMessage: 서로 안부를 묻고 답하는 대화 히스토리 주입\n",
        "      (\"human\", \"Hello, how are you doing?\"),\n",
        "      (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    #HumanMessage로 사용자가 입력한 프롬프트를 전달\n",
        "      (\"human\", \"{user_input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFA3HFH1EACc",
        "outputId": "45d3106d-30d5-4ecf-e870-3eb5cb51e1d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LCEL로 Chain 구축하기"
      ],
      "metadata": {
        "id": "BH2RU-G4IS8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#프롬프트 템플릿 설정\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "\n",
        "#LLM 호출\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "#LCEL로 프롬프트템플릿-LLM-출력 파서 연결하기\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "#invoke함수로 chain 실행하기\n",
        "chain.invoke({\"topic\": \"ice cream\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sK9BDUjkIam3",
        "outputId": "f1e7518f-4ddb-43c5-cb6c-682280cab7a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the ice cream go to school? \\n\\nBecause it wanted to be a little smarter!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chain 선언\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "chain = prompt | model\n",
        "\n",
        "#Chain의 stream()함수를 통해 스트리밍 기능 추가\n",
        "for s in chain.stream({\"topic\": \"bears\"}):\n",
        "  print(s.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21d2AS5xJrud",
        "outputId": "b30facca-1e22-4ed2-b1b5-edfc73ba017c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do bears have hairy coats?  \n",
            "\n",
            "Because they look silly in sweaters!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OutputParser 실습"
      ],
      "metadata": {
        "id": "ZBJBoD9RLiQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature = 0)\n",
        "\n",
        "#ChatPromptTemplate에 SystemMessage로 LLM의 역할과 출력 형식 지정\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"너는 영화 전문가 AI야. 사용자가 원하는 장르의 영화를 리스트 형태로 추천해줘.\"\n",
        "                'ex) Query: SF영화 3개 추천해줘 / 답변: [\"인터스텔라\", \"스페이스오디세이\", \"혹성탈출\"]'\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "\n",
        "    ]\n",
        ")\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "chain = chat_template | model\n",
        "chain.invoke(\"액션\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzmCeIrfLlx1",
        "outputId": "182f1df1-5188-493c-fa08-307676f06286"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='[\"다크 나이트\", \"매드맥스: 분노의 도로\", \"존 윅\"]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 72, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-381e538a-2ba1-4843-a0a5-153bdf807136-0', usage_metadata={'input_tokens': 72, 'output_tokens': 24, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "#CSV 파서 선언\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "#CSV 파서 작동을 위한 형식 지정 프롬프트 로드\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "#프롬프트 템플릿의 partial_variables에 CSV 형식 지정 프롬프트 주입\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List {subject}. answer in korean \\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "#프롬프트템플릿-모델-Output Parser를 체인으로 연결\n",
        "chain = prompt | model | output_parser\n",
        "chain.invoke({\"subject\": \"공포 영화\"})"
      ],
      "metadata": {
        "id": "p7G549hUR9Fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a1ab40-950c-456d-f996-6fc8db983277"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사냥', '원룸', '곡성', '주온', '미스트', '샌드맨', '더 툼즈', '공포의 집', '악마를 보았다', '철수의 방']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 데이터 구조를 정의합니다.\n",
        "class Country(BaseModel):\n",
        "  continent: str = Field(description=\"사용자가 물어본 나라가 속한 대륙\")\n",
        "  population: str = Field(description=\"사용자가 물어본 나라의 인구(int 형식)\")\n",
        "\n",
        "# JsonOutputParser를 설정하고 프롬프트 템플릿에 fromat_instructions를 삽입합니다.\n",
        "parser = JsonOutputParser(pydantic_object=Country)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "chain = prompt | model | parser\n",
        "\n",
        "country_query = \"아르헨티나는 어떤 나라야?\"\n",
        "chain.invoke({\"query\": country_query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oLz5y2ydzcX",
        "outputId": "8def88d3-0fa0-4680-d7e7-73cdbdcb5d51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'continent': 'South America', 'population': '45195777'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LCEL의 Runnable 객체에 대해 알아보기 1"
      ],
      "metadata": {
        "id": "oHhbiH11pk2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnablePassthrough 알아보기"
      ],
      "metadata": {
        "id": "2bn11V4Cps_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "RunnablePassthrough().invoke(\"안녕하세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rr1LtjEgpxaz",
        "outputId": "8678ac5a-4239-4934-c9fb-b31c82424904"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"다음 한글 문장을 프랑스어로 번역해줘 {sentence}\n",
        "French sentence: (print from here)\"\"\")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "runnable_chain = {\"sentence\": RunnablePassthrough()} | prompt | model | output_parser\n",
        "runnable_chain.invoke({\"sentence\": \"그녀는 매일 아침 책을 읽습니다.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RW1YHOcNpx9m",
        "outputId": "f6c16479-769d-43da-d0f8-a6d2f4fdef8d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Elle lit un livre chaque matin.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(RunnablePassthrough.assign(mult=lambda x: x[\"num\"]*3)).invoke({\"num\":3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I--AX3NWsVNc",
        "outputId": "b7b1cfb7-62da-49d2-9488-ec2aaed036b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num': 3, 'mult': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"]*3),\n",
        "    modified=lambda x: x[\"num\"] + 1,\n",
        ")\n",
        "\n",
        "runnable.invoke({\"num\": 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU-g_Tu1sVGL",
        "outputId": "8328063f-4e32-4177-c5e6-8166bbfe7e87"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnableLambda 알아보기"
      ],
      "metadata": {
        "id": "7qV4EmhfBfvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_smile(x):\n",
        "  return x + \":)\""
      ],
      "metadata": {
        "id": "GU4192r7BgEM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "add_smile = RunnableLambda(add_smile)"
      ],
      "metadata": {
        "id": "jOB2R78WBgXC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "prompt_str = \"{topic}의 역사에 대해 세문장으로 설명해주세요.\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "model = ChatOpenAI(model_name = 'gpt-4o-mini')\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "SEy7BZY9BgNN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def add_thank(x):\n",
        "  return x + \" 들어주셔서 감사합니다 :)\"\n",
        "\n",
        "add_thank = RunnableLambda(add_thank)"
      ],
      "metadata": {
        "id": "-wXrfidMC85U"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | output_parser | add_thank\n",
        "chain.invoke(\"반도체\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "zdjk1Hk6C9OK",
        "outputId": "10cae19c-0f49-49ab-ffa1-d8b6f3496995"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'반도체는 20세기 중반에 트랜지스터의 발명으로 급속히 발전하기 시작했으며, 이로 인해 전자기기의 소형화와 고성능화가 가능해졌다. 1970년대에는 집적회로(ICC)가 등장하면서 반도체 산업이 비약적으로 성장하게 되었고, 이는 컴퓨터와 모바일 기기의 발전으로 이어졌다. 현재 반도체는 인공지능, IoT(사물인터넷) 등 다양한 분야에서 핵심 기술로 자리매김하고 있다. 들어주셔서 감사합니다 :)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnableParallel 알아보기"
      ],
      "metadata": {
        "id": "HQ8ffRAqDwAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnableAssign\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    modified=lambda x: x[\"num\"] + 1,\n",
        ")\n",
        "\n",
        "runnable.invoke({\"num\": 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeFsq_QYDwcE",
        "outputId": "a25c9f13-86d9-47c8-d31f-bf870b0ac872"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'passed': {'num': 1}, 'modified': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    modified=add_thank,\n",
        ")\n",
        "\n",
        "runnable.invoke(\"안녕하세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcZ7DI1JDwpT",
        "outputId": "fbc34336-380e-484b-dcb2-27546a29de26"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'passed': '안녕하세요', 'modified': '안녕하세요 들어주셔서 감사합니다 :)'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "model = ChatOpenAI(model = 'gpt-4o-mini', max_tokens = 128, temperature = 0)\n",
        "\n",
        "history_prompt = ChatPromptTemplate.from_template(\"{topic}가 무엇의 약자인지 알려주세요.\")\n",
        "celeb_prompt = ChatPromptTemplate.from_template(\"{topic} 분야의 유명인사 3명의 이름만 알려주세요.\")\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "history_chain = history_prompt | model | output_parser\n",
        "celeb_chain = celeb_prompt | model | output_parser\n",
        "\n",
        "map_chain = RunnableParallel(history=history_chain, celeb=celeb_chain)\n",
        "\n",
        "result = map_chain.invoke({\"topic\": \"AI\"})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2t1YCSvEAaJ",
        "outputId": "939eeff6-86bb-4797-e026-60edacf00965"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI는 \"Artificial Intelligence\"의 약자로, 한국어로는 \"인공지능\"이라고 합니다. 인공지능은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 추론, 문제 해결, 언어 이해 등의 작업을 수행할 수 있도록 하는 기술을 의미합니다.',\n",
              " 'celeb': '1. 앤드류 응 (Andrew Ng)\\n2. 제프리 힌튼 (Geoffrey Hinton)\\n3. 얀 르쿤 (Yann LeCun)'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}